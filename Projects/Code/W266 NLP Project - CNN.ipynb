{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Project\n",
    "\n",
    "### Adam Sayre & Erin Werner\n",
    "\n",
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,r\"./anaconda3/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd \n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import emoji\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "assert(nltk.download(\"treebank\"))\n",
    "from nltk.corpus import europarl_raw\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                            Content  \\\n",
       "0  disappointed  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1  disappointed                       i feel nor am i shamed by it   \n",
       "2  disappointed  i had been feeling a little bit defeated by th...   \n",
       "3         happy  imagine if that reaction guy that called jj kf...   \n",
       "4  disappointed  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...  \n",
       "1                       i feel nor am i shamed by it  \n",
       "2  i had been feeling a little bit defeated by th...  \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...  \n",
       "4  i wouldnt feel burdened so that i would live m...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data.csv\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "      <th>E_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "      <td>rt usertaginstance usertaginstance oh fuck wro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>feel shamed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>feeling little bit defeated steps faith would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "      <td>usertaginstance imagine reaction guy called jj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>wouldnt feel burdened would live life testamen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                            Content  \\\n",
       "0  disappointed  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1  disappointed                       i feel nor am i shamed by it   \n",
       "2  disappointed  i had been feeling a little bit defeated by th...   \n",
       "3         happy  imagine if that reaction guy that called jj kf...   \n",
       "4  disappointed  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \\\n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...   \n",
       "1                       i feel nor am i shamed by it   \n",
       "2  i had been feeling a little bit defeated by th...   \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...   \n",
       "4  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                           E_Content  \n",
       "0  rt usertaginstance usertaginstance oh fuck wro...  \n",
       "1                                        feel shamed  \n",
       "2  feeling little bit defeated steps faith would ...  \n",
       "3  usertaginstance imagine reaction guy called jj...  \n",
       "4  wouldnt feel burdened would live life testamen...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_e = pd.read_csv(\"dataset(clean)_e.csv\") \n",
    "data_e.head()[['Emotion','Content','Original Content','E_Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "      <th>A_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "      <td>b rt davbingodav mcrackins oh fuck wrote fil g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>feel shamed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>feeling little bit defeated steps faith would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "      <td>b ksiolajidebt imagine reaction guy called jj ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>wouldnt feel burdened would live life testamen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                            Content  \\\n",
       "0  disappointed  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1  disappointed                       i feel nor am i shamed by it   \n",
       "2  disappointed  i had been feeling a little bit defeated by th...   \n",
       "3         happy  imagine if that reaction guy that called jj kf...   \n",
       "4  disappointed  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \\\n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...   \n",
       "1                       i feel nor am i shamed by it   \n",
       "2  i had been feeling a little bit defeated by th...   \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...   \n",
       "4  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                           A_Content  \n",
       "0  b rt davbingodav mcrackins oh fuck wrote fil g...  \n",
       "1                                        feel shamed  \n",
       "2  feeling little bit defeated steps faith would ...  \n",
       "3  b ksiolajidebt imagine reaction guy called jj ...  \n",
       "4  wouldnt feel burdened would live life testamen...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_a = pd.read_csv(\"dataset(clean)_a.csv\") \n",
    "data_a.head()[['Emotion','Content','Original Content','A_Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow CNN\n",
    "\n",
    "#### Original Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disappointed': 0, 'happy': 1, 'angry': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = data.Emotion.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data.Emotion.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(data.Content.values, data.label.values, test_size=0.3, \n",
    "                                                  random_state=42, stratify=data.label.values)\n",
    "\n",
    "#X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(X_train_c, y_train_c, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<641602x137520 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7220343 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_c = CountVectorizer()\n",
    "vectorizer_c.fit(X_train_c)\n",
    "\n",
    "X_train_c = vectorizer_c.transform(X_train_c)\n",
    "X_val_c  = vectorizer_c.transform(X_val_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Uncleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Original_Content\"] = data[\"Original Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oc, X_val_oc, y_train_oc, y_val_oc = train_test_split(data.Original_Content.values, data.label.values, \n",
    "                                                  test_size=0.3, random_state=42, stratify=data.label.values)\n",
    "\n",
    "#X_train_oc, X_val_oc, y_train_oc, y_val_oc = train_test_split(X_train_oc, y_train_oc, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_oc = CountVectorizer()\n",
    "vectorizer_oc.fit(X_train_oc)\n",
    "\n",
    "X_train_oc = vectorizer_oc.transform(X_train_oc)\n",
    "X_val_oc  = vectorizer_oc.transform(X_val_oc)\n",
    "X_train_oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Cleaned Data #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels_e = data_e.Emotion.unique()\n",
    "\n",
    "label_dict_e = {}\n",
    "for index, possible_label in enumerate(possible_labels_e):\n",
    "    label_dict_e[possible_label] = index\n",
    "\n",
    "data_e['label'] = data_e.Emotion.replace(label_dict_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_e, X_val_e, y_train_e, y_val_e = train_test_split(data_e.E_Content.values, data_e.label.values, \n",
    "                                                  test_size=0.3,random_state=42, stratify=data_e.label.values)\n",
    "\n",
    "#X_train_e, X_val_e, y_train_e, y_val_e = train_test_split(X_train_e, y_train_e, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_e = CountVectorizer()\n",
    "vectorizer_e.fit(X_train_e)\n",
    "\n",
    "X_train_e = vectorizer_e.transform(X_train_e)\n",
    "X_val_e  = vectorizer_e.transform(X_val_e)\n",
    "X_train_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Cleaned Data #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels_a = data_a.Emotion.unique()\n",
    "\n",
    "label_dict_a = {}\n",
    "for index, possible_label in enumerate(possible_labels_a):\n",
    "    label_dict_a[possible_label] = index\n",
    "\n",
    "data_a['label'] = data_a.Emotion.replace(label_dict_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(data_a.A_Content.values, data_a.label.values, \n",
    "                                                  test_size=0.3,random_state=42, stratify=data_a.label.values)\n",
    "\n",
    "#X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_train_a, y_train_a, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_a = CountVectorizer()\n",
    "vectorizer_a.fit(X_train_a)\n",
    "\n",
    "X_train_a = vectorizer_a.transform(X_train_a)\n",
    "X_val_a  = vectorizer_a.transform(X_val_a)\n",
    "X_train_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = layers.Input(shape=(length,))\n",
    "    embedding1 = layers.Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = layers.Dropout(0.5)(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = layers.Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = layers.Input(shape=(length,))\n",
    "    embedding2 = layers.Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = layers.Dropout(0.5)(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = layers.Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = layers.Input(shape=(length,))\n",
    "    embedding3 = layers.Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = layers.Dropout(0.5)(conv3)\n",
    "    pool3 = layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = layers.Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = layers.concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = layers.Dense(10, activation='relu')(merged)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tc, X_val_tc, y_train_tc, y_val_tc = train_test_split(data.Content.values, data.label.values, test_size=0.3, \n",
    "                                                  random_state=42, stratify=data.label.values)\n",
    "\n",
    "#X_train_tc, X_val_tc, y_train_tc, y_val_tc = train_test_split(X_train_tc, y_train_tc, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(X_train_tc)\n",
    "# calculate max document length\n",
    "length = max_length(X_train_tc)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX_tc = encode_text(tokenizer, X_train_tc, length)\n",
    "valX_tc = encode_text(tokenizer, X_val_tc, length)\n",
    "print(trainX_tc.shape, valX_tc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX_tc,trainX_tc,trainX_tc], y_train_tc, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn_loss_tc, dcnn_accuracy_tc = model.evaluate([valX_tc,valX_tc,valX_tc], y_val_tc, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Uncleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_toc, X_val_toc, y_train_toc, y_val_toc = train_test_split(data.Original_Content.values, \n",
    "                                                  data.label.values, test_size=0.3, \n",
    "                                                  random_state=42, stratify=data.label.values)\n",
    "\n",
    "#X_train_toc, X_val_toc, y_train_toc, y_val_toc = train_test_split(X_train_toc, y_train_toc, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(X_train_toc)\n",
    "# calculate max document length\n",
    "length = max_length(X_train_toc)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX_toc = encode_text(tokenizer, X_train_toc, length)\n",
    "valX_toc = encode_text(tokenizer, X_val_toc, length)\n",
    "print(trainX_toc.shape, valX_toc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX_toc,trainX_toc,trainX_toc], y_train_toc, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn_loss_toc, dcnn_accuracy_toc = model.evaluate([valX_toc,valX_toc,valX_toc], y_val_toc, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Cleaned Data #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_te, X_val_te, y_train_te, y_val_te = train_test_split(data_e.E_Content.values, \n",
    "                                                  data_e.label.values, test_size=0.3, \n",
    "                                                  random_state=42, stratify=data_e.label.values)\n",
    "\n",
    "#X_train_te, X_val_te, y_train_te, y_val_te = train_test_split(X_train_te, y_train_te, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(X_train_te)\n",
    "# calculate max document length\n",
    "length = max_length(X_train_te)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX_te = encode_text(tokenizer, X_train_te, length)\n",
    "valX_te = encode_text(tokenizer, X_val_te, length)\n",
    "print(trainX_te.shape, valX_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX_te,trainX_te,trainX_te], y_train_te, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn_loss_te, dcnn_accuracy_te = model.evaluate([valX_te,valX_te,valX_te], y_val_te, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Cleaned Data #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ta, X_val_ta, y_train_ta, y_val_ta = train_test_split(data_a.A_Content.values, \n",
    "                                                  data_a.label.values, test_size=0.3, \n",
    "                                                  random_state=42, stratify=data_a.label.values)\n",
    "\n",
    "#X_train_ta, X_val_ta, y_train_ta, y_val_ta = train_test_split(X_train_ta, y_train_ta, test_size=0.5, \n",
    "#                                                  random_state=42, stratify=y_train_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(X_train_ta)\n",
    "# calculate max document length\n",
    "length = max_length(X_train_ta)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX_ta = encode_text(tokenizer, X_train_ta, length)\n",
    "valX_ta = encode_text(tokenizer, X_val_ta, length)\n",
    "print(trainX_ta.shape, valX_ta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX_ta,trainX_ta,trainX_ta], y_train_ta, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn_loss_ta, dcnn_accuracy_ta = model.evaluate([valX_ta,valX_ta,valX_ta], y_val_ta, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnn_acc_train = [dcnn_accuracy_toc, dcnn_accuracy_tc, dcnn_accuracy_te, dcnn_accuracy_ta]\n",
    "dcnn_loss_train = [dcnn_loss_toc, dcnn_loss_tc, dcnn_loss_te, dcnn_loss_ta]\n",
    "dcnn_values = ['Orig. Uncleaned', 'Orig. Cleaned', 'Custom Cleaned #1', 'Custom Cleaned #2']\n",
    "dcnn_df = pd.DataFrame()\n",
    "dcnn_df['Cleaning Method'] = dcnn_values\n",
    "dcnn_df['F1 Score'] = dcnn_acc_train\n",
    "dcnn_df['Loss'] = dcnn_loss_train\n",
    "dcnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_p37",
   "language": "python",
   "name": "tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
